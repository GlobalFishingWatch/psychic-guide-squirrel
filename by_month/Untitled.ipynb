{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tools for making queries, moving the results to GCS and then\n",
    "downloading via gsutil.\n",
    "Here's how I use it:\n",
    "query_job = bigq.async_query(proj_id, query, dataset, table)\n",
    "bigq.poll_job(query_job)\n",
    "extract_job = bigq.async_extract_query(job, gcs_path)\n",
    "bigq.poll_job(extract_job)\n",
    "gs_mv(gcs_path, local_path)\n",
    "\"\"\"\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "\n",
    "\n",
    "class BigQuery:\n",
    "\n",
    "    def __init__(self):\n",
    "        credentials = GoogleCredentials.get_application_default()\n",
    "        self._bq = discovery.build('bigquery', 'v2', credentials=credentials)\n",
    "\n",
    "\n",
    "    # XXX allow dataset/table to be optional (should probably pass in as atomic `destination`)\n",
    "    # XXXX allow allowLargeResults to be optional\n",
    "    # XXX check that dataset/table set if not\n",
    "    # XXX ADD note that if dest table specified it is not automatically deleted\n",
    "    def async_query(self, project_id, query, dataset, table,\n",
    "                        batch=False, num_retries=5):\n",
    "        \"\"\"Create an asynchronous BigQuery query\n",
    "        MOAR DOCS\n",
    "        \"\"\"\n",
    "        # Generate a unique job_id so retries\n",
    "        # don't accidentally duplicate query\n",
    "        job_data = {\n",
    "            'jobReference': {\n",
    "                'projectId': project_id,\n",
    "                'job_id': str(uuid.uuid4())\n",
    "            },\n",
    "            'configuration': {\n",
    "                'query': {\n",
    "                    'allowLargeResults': 'true',\n",
    "                    'destinationTable' : {\n",
    "                      \"projectId\": project_id,\n",
    "                      \"datasetId\": dataset,\n",
    "                      \"tableId\": table,\n",
    "                      },\n",
    "                    'query': query,\n",
    "                    'priority': 'BATCH' if batch else 'INTERACTIVE'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return self._bq.jobs().insert(\n",
    "            projectId=project_id,\n",
    "            body=job_data).execute(num_retries=num_retries)\n",
    "\n",
    "\n",
    "    def poll_job(self, job, max_tries=4000):\n",
    "        \"\"\"Waits for a job to complete.\"\"\"\n",
    "\n",
    "        request = self._bq.jobs().get(\n",
    "            projectId=job['jobReference']['projectId'],\n",
    "            jobId=job['jobReference']['jobId'])\n",
    "\n",
    "        trial = 0\n",
    "        while trial < max_tries:\n",
    "            result = request.execute(num_retries=2)\n",
    "\n",
    "            if result['status']['state'] == 'DONE':\n",
    "                if 'errorResult' in result['status']:\n",
    "                    raise RuntimeError(result['status']['errorResult'])\n",
    "                return\n",
    "\n",
    "            time.sleep(1)\n",
    "            trial += 1\n",
    "\n",
    "        raise RuntimeError(\"timeout\")\n",
    "\n",
    "\n",
    "    def async_extract_query(self, job, path, format=\"CSV\", compression=\"GZIP\",\n",
    "                                                        num_retries=5):\n",
    "        \"\"\"Extracts query specified by job into Google Cloud storage at path\n",
    "        MOAR docs\n",
    "        \"\"\"\n",
    "\n",
    "        job_data = {\n",
    "          'jobReference': {\n",
    "              'projectId': job['jobReference']['projectId'],\n",
    "              'jobId': str(uuid.uuid4())\n",
    "          },\n",
    "          'configuration': {\n",
    "              'extract': {\n",
    "                  'sourceTable': {\n",
    "                      'projectId': job['configuration']['query']['destinationTable']['projectId'],\n",
    "                      'datasetId': job['configuration']['query']['destinationTable']['datasetId'],\n",
    "                      'tableId': job['configuration']['query']['destinationTable']['tableId'],\n",
    "                  },\n",
    "                  'destinationUris': [path],\n",
    "                  'destinationFormat': format,\n",
    "                  'compression': compression\n",
    "              }\n",
    "          }\n",
    "        }\n",
    "        return self._bq.jobs().insert(\n",
    "            projectId=job['jobReference']['projectId'],\n",
    "            body=job_data).execute(num_retries=num_retries)\n",
    "\n",
    "\n",
    "def gs_mv(src_path, dest_path):\n",
    "    \"\"\"Move data using gsutil\n",
    "    This was written to move data from cloud\n",
    "    storage down to your computer and hasn't been\n",
    "    tested for other things.\n",
    "    Example:\n",
    "    gs_mv(\"gs://world-fishing-827/scratch/SOME_DIR/SOME_FILE\",\n",
    "                \"some/local/path/.\")\n",
    "    \"\"\"\n",
    "    subprocess.call([\"gsutil\", \"-m\", \"mv\", src_path, dest_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
