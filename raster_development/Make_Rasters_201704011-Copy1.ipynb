{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import gzip\n",
    "import rasterio as rio\n",
    "import affine\n",
    "import os.path\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class BigQuery:\n",
    "\n",
    "    def __init__(self):\n",
    "        credentials = GoogleCredentials.get_application_default()\n",
    "        self._bq = discovery.build('bigquery', 'v2', credentials=credentials)\n",
    "\n",
    "\n",
    "    # XXX allow dataset/table to be optional (should probably pass in as atomic `destination`)\n",
    "    # XXXX allow allowLargeResults to be optional\n",
    "    # XXX check that dataset/table set if not\n",
    "    # XXX ADD note that if dest table specified it is not automatically deleted\n",
    "    def async_query(self, project_id, query, dataset, table,\n",
    "                        batch=False, num_retries=5):\n",
    "        \"\"\"Create an asynchronous BigQuery query\n",
    "        MOAR DOCS\n",
    "        \"\"\"\n",
    "        # Generate a unique job_id so retries\n",
    "        # don't accidentally duplicate query\n",
    "        job_data = {\n",
    "            'jobReference': {\n",
    "                'projectId': project_id,\n",
    "                'job_id': str(uuid.uuid4())\n",
    "            },\n",
    "            'configuration': {\n",
    "                'query': {\n",
    "                    'allowLargeResults': 'true',\n",
    "                    'writeDisposition':'WRITE_TRUNCATE', #overwrites table\n",
    "                    'destinationTable' : {\n",
    "                      \"projectId\": project_id,\n",
    "                      \"datasetId\": dataset,\n",
    "                      \"tableId\": table,\n",
    "                      },\n",
    "                    'query': query,\n",
    "                    'priority': 'BATCH' if batch else 'INTERACTIVE'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return self._bq.jobs().insert(\n",
    "            projectId=project_id,\n",
    "            body=job_data).execute(num_retries=num_retries)\n",
    "\n",
    "\n",
    "    def poll_job(self, job, max_tries=4000):\n",
    "        \"\"\"Waits for a job to complete.\"\"\"\n",
    "\n",
    "        request = self._bq.jobs().get(\n",
    "            projectId=job['jobReference']['projectId'],\n",
    "            jobId=job['jobReference']['jobId'])\n",
    "\n",
    "        trial = 0\n",
    "        while trial < max_tries:\n",
    "            result = request.execute(num_retries=2)\n",
    "\n",
    "            if result['status']['state'] == 'DONE':\n",
    "                if 'errorResult' in result['status']:\n",
    "                    raise RuntimeError(result['status']['errorResult'])\n",
    "                return\n",
    "\n",
    "            time.sleep(1)\n",
    "            trial += 1\n",
    "\n",
    "        raise RuntimeError(\"timeout\")\n",
    "\n",
    "\n",
    "    def async_extract_query(self, job, path, format=\"CSV\", compression=\"GZIP\",\n",
    "                                                        num_retries=5):\n",
    "        \"\"\"Extracts query specified by job into Google Cloud storage at path\n",
    "        MOAR docs\n",
    "        \"\"\"\n",
    "\n",
    "        job_data = {\n",
    "          'jobReference': {\n",
    "              'projectId': job['jobReference']['projectId'],\n",
    "              'jobId': str(uuid.uuid4())\n",
    "          },\n",
    "          'configuration': {\n",
    "              'extract': {\n",
    "                  'sourceTable': {\n",
    "                      'projectId': job['configuration']['query']['destinationTable']['projectId'],\n",
    "                      'datasetId': job['configuration']['query']['destinationTable']['datasetId'],\n",
    "                      'tableId': job['configuration']['query']['destinationTable']['tableId'],\n",
    "                  },\n",
    "                  'destinationUris': [path],\n",
    "                  'destinationFormat': format,\n",
    "                  'compression': compression\n",
    "              }\n",
    "          }\n",
    "        }\n",
    "        return self._bq.jobs().insert(\n",
    "            projectId=job['jobReference']['projectId'],\n",
    "            body=job_data).execute(num_retries=num_retries)\n",
    "\n",
    "\n",
    "def gs_mv(src_path, dest_path):\n",
    "    \"\"\"Move data using gsutil\n",
    "    This was written to move data from cloud\n",
    "    storage down to your computer and hasn't been\n",
    "    tested for other things.\n",
    "    Example:\n",
    "    gs_mv(\"gs://world-fishing-827/scratch/SOME_DIR/SOME_FILE\",\n",
    "                \"some/local/path/.\")\n",
    "    \"\"\"\n",
    "    subprocess.call([\"gsutil\", \"-m\", \"mv\", src_path, dest_path])\n",
    "\n",
    "\n",
    "# this function is to get the approximate area of a grid cell, \n",
    "# which can be used to normalize density. It currently isn't used\n",
    "def get_area(lat):\n",
    "    lat_degree = 69 # miles\n",
    "    # Convert latitude and longitude to \n",
    "    # spherical coordinates in radians.\n",
    "    degrees_to_radians = math.pi/180.0        \n",
    "    # phi = 90 - latitude\n",
    "    phi = (lat+cellsize/2.)*degrees_to_radians #plus half a cell size to get the middle\n",
    "    lon_degree = math.cos(phi)*lat_degree \n",
    "    # return 69*69*2.6\n",
    "    return  lat_degree*lon_degree* 2.58999 # miles to square km\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Execute the query, move the table to gcs, then download it\n",
    "# as a zipped csv file\n",
    "\n",
    "proj_id = \"world-fishing-827\"\n",
    "dataset = \"scratch_global_fishing_raster\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170407\n",
      "20170406\n",
      "20170405\n",
      "20170404\n",
      "20170403\n",
      "20170402\n",
      "20170401\n",
      "20170331\n",
      "20170330\n",
      "20170329\n",
      "20170328\n",
      "20170327\n",
      "20170326\n",
      "20170325\n",
      "20170324\n",
      "20170323\n",
      "20170322\n",
      "20170321\n",
      "20170320\n",
      "20170319\n",
      "20170318\n",
      "20170317\n",
      "20170316\n",
      "20170315\n",
      "20170314\n",
      "20170313\n",
      "20170312\n",
      "20170311\n",
      "20170310\n",
      "20170309\n",
      "20170308\n",
      "20170307\n",
      "20170306\n",
      "20170305\n",
      "20170304\n",
      "20170303\n",
      "20170302\n",
      "20170301\n",
      "20170228\n",
      "20170227\n",
      "20170226\n",
      "20170225\n",
      "20170224\n",
      "20170223\n",
      "20170222\n",
      "20170221\n",
      "20170220\n",
      "20170219\n",
      "20170218\n",
      "20170217\n",
      "20170216\n",
      "20170215\n",
      "20170214\n",
      "20170213\n",
      "20170212\n",
      "20170211\n",
      "20170210\n",
      "20170209\n",
      "20170208\n",
      "20170207\n",
      "20170206\n",
      "20170205\n",
      "20170204\n",
      "20170203\n",
      "20170202\n",
      "20170201\n",
      "20170131\n",
      "20170130\n",
      "20170129\n",
      "20170128\n",
      "20170127\n",
      "20170126\n",
      "20170125\n",
      "20170124\n",
      "20170123\n",
      "20170122\n",
      "20170121\n",
      "20170120\n",
      "20170119\n",
      "20170118\n",
      "20170117\n",
      "20170116\n",
      "20170115\n",
      "20170114\n",
      "20170113\n",
      "20170112\n",
      "20170111\n",
      "20170110\n",
      "20170109\n",
      "20170108\n",
      "20170107\n",
      "20170106\n",
      "20170105\n",
      "20170104\n",
      "20170103\n",
      "20170102\n",
      "20170101\n",
      "20161231\n",
      "20161230\n",
      "20161229\n",
      "20161228\n",
      "20161227\n",
      "20161226\n",
      "20161225\n",
      "20161224\n",
      "20161223\n",
      "20161222\n",
      "20161221\n",
      "20161220\n",
      "20161219\n",
      "20161218\n",
      "20161217\n",
      "20161216\n",
      "20161215\n",
      "20161214\n",
      "20161213\n",
      "20161212\n",
      "20161211\n",
      "20161210\n",
      "20161209\n",
      "20161208\n",
      "20161207\n",
      "20161206\n",
      "20161205\n",
      "20161204\n",
      "20161203\n",
      "20161202\n",
      "20161201\n",
      "20161130\n",
      "20161129\n",
      "20161128\n",
      "20161127\n",
      "20161126\n",
      "20161125\n",
      "20161124\n",
      "20161123\n",
      "20161122\n",
      "20161121\n",
      "20161120\n",
      "20161119\n",
      "20161118\n",
      "20161117\n",
      "20161116\n",
      "20161115\n",
      "20161114\n",
      "20161113\n",
      "20161112\n",
      "20161111\n",
      "20161110\n",
      "20161109\n",
      "20161108\n",
      "20161107\n",
      "20161106\n",
      "20161105\n",
      "20161104\n",
      "20161103\n",
      "20161102\n",
      "20161101\n",
      "20161031\n",
      "20161030\n",
      "20161029\n",
      "20161028\n",
      "20161027\n",
      "20161026\n",
      "20161025\n",
      "20161024\n",
      "20161023\n",
      "20161022\n",
      "20161021\n",
      "20161020\n",
      "20161019\n",
      "20161018\n",
      "20161017\n",
      "20161016\n",
      "20161015\n",
      "20161014\n",
      "20161013\n",
      "20161012\n",
      "20161011\n",
      "20161010\n",
      "20161009\n",
      "20161008\n",
      "20161007\n",
      "20161006\n",
      "20161005\n",
      "20161004\n",
      "20161003\n",
      "20161002\n",
      "20161001\n",
      "20160930\n",
      "20160929\n",
      "20160928\n",
      "20160927\n",
      "20160926\n",
      "20160925\n",
      "20160924\n",
      "20160923\n",
      "20160922\n",
      "20160921\n",
      "20160920\n",
      "20160919\n",
      "20160918\n",
      "20160917\n",
      "20160916\n",
      "20160915\n",
      "20160914\n",
      "20160913\n",
      "20160912\n",
      "20160911\n",
      "20160910\n",
      "20160909\n",
      "20160908\n",
      "20160907\n",
      "20160906\n",
      "20160905\n",
      "20160904\n",
      "20160903\n",
      "20160902\n",
      "20160901\n",
      "20160831\n",
      "20160830\n",
      "20160829\n",
      "20160828\n",
      "20160827\n",
      "20160826\n",
      "20160825\n",
      "20160824\n",
      "20160823\n",
      "20160822\n",
      "20160821\n",
      "20160820\n",
      "20160819\n",
      "20160818\n",
      "20160817\n",
      "20160816\n",
      "20160815\n",
      "20160814\n",
      "20160813\n",
      "20160812\n",
      "20160811\n",
      "20160810\n",
      "20160809\n",
      "20160808\n",
      "20160807\n",
      "20160806\n",
      "20160805\n",
      "20160804\n",
      "20160803\n",
      "20160802\n",
      "20160801\n",
      "20160731\n",
      "20160730\n",
      "20160729\n",
      "20160728\n",
      "20160727\n",
      "20160726\n",
      "20160725\n",
      "20160724\n",
      "20160723\n",
      "20160722\n",
      "20160721\n",
      "20160720\n",
      "20160719\n",
      "20160718\n",
      "20160717\n",
      "20160716\n",
      "20160715\n",
      "20160714\n",
      "20160713\n",
      "20160712\n",
      "20160711\n",
      "20160710\n",
      "20160709\n",
      "20160708\n",
      "20160707\n",
      "20160706\n",
      "20160705\n",
      "20160704\n",
      "20160703\n",
      "20160702\n",
      "20160701\n",
      "20160630\n",
      "20160629\n",
      "20160628\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-44ca6d21d500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mout_tif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_tif\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miso3\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0myyyy\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".tif\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mrio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tif\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trawlers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                     \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'drifting_longlines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'purse_seines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "d = datetime(2017,4,7)\n",
    "\n",
    "for i in range(365):\n",
    "    # print d + timedelta(days=i)\n",
    "    thedate = yyyymmdd = datetime.strftime(d + timedelta(days=-i),\"%Y%m%d\")\n",
    "#     command = '''python Make_Rasters_201704011.py {yyyymmdd}'''.format(yyyymmdd=yyyymmdd)\n",
    "#     commands.append(command)\n",
    "#     thedate = \"20150103\" \n",
    "\n",
    "\n",
    "    path_to_csv_zip = \"data/dailytables/fishing_vessel_effort_iso3_gears/zips/\"\n",
    "    path_to_tif =   \"data/dailytables/fishing_vessel_effort_iso3_gears/tifs/\"\n",
    "    path_to_csv_zip_iso3 = \"data/dailytables/fishing_vessel_effort_iso3_gears/iso3s/\"\n",
    "\n",
    "    # codes = {}\n",
    "    # all_codes = []\n",
    "    # with open('iso3.csv','rU') as csvfile:\n",
    "    #     reader = csv.DictReader(csvfile)\n",
    "    #     for row in reader:\n",
    "    #         iso3 = row['iso3']\n",
    "    #         code = row['code']\n",
    "    #         if iso3 not in codes:\n",
    "    #             codes[iso3] = []\n",
    "    #         codes[iso3].append(code)\n",
    "    #         all_codes.append(code)\n",
    "\n",
    "\n",
    "    yyyy = thedate[:4]\n",
    "    mm = thedate[4:6]\n",
    "    dd = thedate[6:8]\n",
    "\n",
    "\n",
    "    query = '''\n",
    "    SELECT\n",
    "      FLOOR(lat*10) lat_bin,\n",
    "      FLOOR(lon*10) lon_bin,\n",
    "      SUM(hours) hours,\n",
    "      if(inferred_label is null, \"unknown\",inferred_label) AS label,\n",
    "      if(iso3 is null, \"UNK\", iso3) as iso3\n",
    "    FROM (\n",
    "      SELECT\n",
    "        mmsi,\n",
    "        lat,\n",
    "        lon,\n",
    "        hours,\n",
    "        flag_iso3 AS iso3\n",
    "      FROM\n",
    "        [world-fishing-827:gfw_research.FAO${thedate}]\n",
    "      WHERE\n",
    "        measure_new_score > .5 \n",
    "        and seg_id NOT IN (\n",
    "        SELECT\n",
    "          seg_id\n",
    "        FROM\n",
    "          [world-fishing-827:gfw_published.segments],\n",
    "        WHERE\n",
    "          valid_positions < 10\n",
    "          AND satellite_positions = 0)) a\n",
    "    LEFT JOIN (\n",
    "      SELECT\n",
    "        mmsi,\n",
    "        inferred_label\n",
    "      FROM\n",
    "        [gfw_research.vessel_info_20170405]\n",
    "      WHERE\n",
    "        year={yyyy}) b\n",
    "    ON\n",
    "      a.mmsi = b.mmsi\n",
    "    GROUP BY\n",
    "      label,\n",
    "      iso3,\n",
    "      lat_bin,\n",
    "      lon_bin\n",
    "    ORDER BY\n",
    "      iso3,\n",
    "      label,\n",
    "    '''.format(thedate=thedate,yyyy=yyyy)\n",
    "\n",
    "\n",
    "    table = \"fishing_effort_\"+thedate\n",
    "    gcs_path = \"gs://david-scratch/\"+table+\".zip\"\n",
    "    local_path = path_to_csv_zip+table+\".zip\"\n",
    "    \n",
    "    print thedate\n",
    "\n",
    "    if not os.path.isfile(local_path): \n",
    "        bigq = BigQuery()\n",
    "        query_job = bigq.async_query(proj_id, query, dataset, table)\n",
    "        bigq.poll_job(query_job)\n",
    "        extract_job = bigq.async_extract_query(query_job, gcs_path)\n",
    "        bigq.poll_job(extract_job)\n",
    "        gs_mv(gcs_path, local_path)\n",
    "\n",
    "    one_over_cellsize = 10\n",
    "    cellsize = .1\n",
    "    num_lats = 180 * one_over_cellsize\n",
    "    num_lons = 360 * one_over_cellsize\n",
    "\n",
    "\n",
    "    profile = {\n",
    "                'crs': 'EPSG:4326',\n",
    "                'nodata': -9999,\n",
    "                'dtype': rio.float32,\n",
    "                'height': num_lats,\n",
    "                'width': num_lons,\n",
    "                'count': 6,\n",
    "                'driver': \"GTiff\",\n",
    "                'transform': affine.Affine(float(cellsize), 0, -180, \n",
    "                                           0, -float(cellsize), 90),\n",
    "                'TILED': 'YES',\n",
    "                'BIGTIFF': 'NO',\n",
    "                'INTERLEAVE': 'BAND',\n",
    "                'COMPRESS': 'DEFLATE',\n",
    "                'PREDICTOR': '3'\n",
    "            }\n",
    "\n",
    "    hours = {}\n",
    "    hours['trawlers'] = np.zeros(shape=(num_lats,num_lons))\n",
    "    hours['drifting_longlines'] = np.zeros(shape=(num_lats,num_lons))\n",
    "    hours['purse_seines'] = np.zeros(shape=(num_lats,num_lons))\n",
    "    hours['fixed_gear'] = np.zeros(shape=(num_lats,num_lons))\n",
    "    hours['squid_jigger'] = np.zeros(shape=(num_lats,num_lons))\n",
    "    hours['other_unknown'] = np.zeros(shape=(num_lats,num_lons))\n",
    "    \n",
    "    with gzip.open(local_path, 'rb') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        \n",
    "        n = 0\n",
    "        for row in reader:\n",
    "            if n == 0:\n",
    "                iso3 = row['iso3']\n",
    "                n = 1\n",
    "            if row['iso3'] != iso3:  \n",
    "#                 print hours['trawlers'].sum()\n",
    "\n",
    "                # # Read bands individually and write individually\n",
    "                # with rio.open('RGB.byte.tif') as src, \\\n",
    "                #         rio.open('individual.tif', 'w', **src.profile) as dst:\n",
    "                #     for bidx in range(1, src.count + 1):\n",
    "                #         data = src.read(bidx)\n",
    "                #         dst.write(data, indexes=bidx)\n",
    "\n",
    "                # Kevin says You also want the GeoTIFF creation option `PHOTOMETRIC=MINISBLACK`.  \n",
    "                # If an image has 3+ bands of type Byte GDAL will assume `PHOTOMETRIC=RGB`\n",
    "                os.system(\"mkdir \"+path_to_tif + iso3 )\n",
    "                out_tif = path_to_tif + iso3 +\"/\"+yyyy+\"-\"+mm+\"-\"+dd+\".tif\"\n",
    "                with rio.open(out_tif, 'w', **profile) as dst:\n",
    "                    dst.write(np.flipud(hours['trawlers']).astype(profile['dtype']), indexes=1)\n",
    "                    dst.write(np.flipud(hours['drifting_longlines']).astype(profile['dtype']), indexes=2)\n",
    "                    dst.write(np.flipud(hours['purse_seines']).astype(profile['dtype']), indexes=3)\n",
    "                    dst.write(np.flipud(hours['fixed_gear'] ).astype(profile['dtype']), indexes=4)\n",
    "                    dst.write(np.flipud(hours['squid_jigger']).astype(profile['dtype']), indexes=5)\n",
    "                    dst.write(np.flipud(hours['other_unknown']).astype(profile['dtype']), indexes=6)\n",
    "                    # dst.write((np.flipud(vessel_hours)/2.).astype(profile['dtype']), indexes=2)\n",
    "\n",
    "                hours = {}\n",
    "                hours['trawlers'] = np.zeros(shape=(num_lats,num_lons))\n",
    "                hours['drifting_longlines'] = np.zeros(shape=(num_lats,num_lons))\n",
    "                hours['purse_seines'] = np.zeros(shape=(num_lats,num_lons))\n",
    "                hours['fixed_gear'] = np.zeros(shape=(num_lats,num_lons))\n",
    "                hours['squid_jigger'] = np.zeros(shape=(num_lats,num_lons))\n",
    "                hours['other_unknown'] = np.zeros(shape=(num_lats,num_lons))\n",
    "                iso3 = row['iso3']\n",
    "\n",
    "            lat = int(row['lat_bin'])\n",
    "            lon = int(row['lon_bin'])   \n",
    "            if lat<90*one_over_cellsize and \\\n",
    "            lat>-90*one_over_cellsize and lon>-180*one_over_cellsize and lon<180*one_over_cellsize:\n",
    "                label = row['label']\n",
    "                if label not in [\"drifting_longlines\",\"fixed_gear\",\"squid_jigger\", \"purse_seines\",\"trawlers\"]:\n",
    "                    label = \"other_unknown\"\n",
    "                lat_index = lat+90*one_over_cellsize\n",
    "                lon_index = lon+180*one_over_cellsize\n",
    "                hours[label][lat_index][lon_index] = float(row['hours'])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
